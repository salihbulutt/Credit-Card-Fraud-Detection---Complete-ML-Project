# Credit Card Fraud Detection - Baseline Model

"""
## Baseline Model Development

This notebook establishes a baseline model for credit card fraud detection.
The baseline serves as a reference point to measure improvements from feature 
engineering and model optimization.

### Objectives:
1. Create a simple, interpretable baseline model
2. Use minimal feature engineering
3. Establish performance benchmarks
4. Identify key areas for improvement
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report, confusion_matrix, 
    roc_auc_score, average_precision_score,
    precision_recall_curve, roc_curve
)
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

## 1. Load and Prepare Data

print("="*70)
print("BASELINE MODEL - CREDIT CARD FRAUD DETECTION")
print("="*70)

# Load data
df = pd.read_csv('../data/raw/creditcard.csv')
print(f"\nLoaded {len(df):,} transactions")
print(f"Fraud rate: {df['Class'].mean():.4%}")

# Separate features and target
X = df.drop(columns=['Class'])
y = df['Class']

# Train-test split (stratified to maintain fraud ratio)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nTrain set: {len(X_train):,} samples")
print(f"Test set: {len(X_test):,} samples")
print(f"Train fraud rate: {y_train.mean():.4%}")
print(f"Test fraud rate: {y_test.mean():.4%}")

## 2. Baseline Feature Set

"""
### Baseline Features:
- All 30 original features (V1-V28, Time, Amount)
- No feature engineering
- Standard scaling only

This minimal approach helps us understand the raw predictive power 
of the dataset before applying sophisticated techniques.
"""

# Drop Time feature (not useful in raw form)
X_train_baseline = X_train.drop(columns=['Time'])
X_test_baseline = X_test.drop(columns=['Time'])

print(f"\nBaseline features: {X_train_baseline.shape[1]}")
print(f"Features: {list(X_train_baseline.columns)}")

## 3. Preprocessing

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_baseline)
X_test_scaled = scaler.transform(X_test_baseline)

print("\n✓ Features standardized using StandardScaler")

## 4. Train Baseline Model

"""
### Model Choice: Logistic Regression
- Simple, interpretable, fast
- Works well as a baseline
- Provides probability estimates
- Class weights to handle imbalance
"""

print("\n" + "="*70)
print("TRAINING BASELINE MODEL")
print("="*70)

# Initialize model with class weights
baseline_model = LogisticRegression(
    random_state=42,
    max_iter=1000,
    class_weight='balanced',  # Handle imbalance
    solver='lbfgs'
)

# Train model
baseline_model.fit(X_train_scaled, y_train)
print("✓ Model trained successfully")

## 5. Make Predictions

# Predict on test set
y_pred = baseline_model.predict(X_test_scaled)
y_pred_proba = baseline_model.predict_proba(X_test_scaled)[:, 1]

print("\n✓ Predictions generated")

## 6. Evaluate Performance

print("\n" + "="*70)
print("BASELINE MODEL PERFORMANCE")
print("="*70)

# Calculate metrics
roc_auc = roc_auc_score(y_test, y_pred_proba)
pr_auc = average_precision_score(y_test, y_pred_proba)

print(f"\nROC-AUC Score: {roc_auc:.4f}")
print(f"PR-AUC Score: {pr_auc:.4f}")

# Classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['Legitimate', 'Fraud']))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()

print("\nConfusion Matrix:")
print(f"True Negatives:  {tn:,}")
print(f"False Positives: {fp:,}")
print(f"False Negatives: {fn:,}")
print(f"True Positives:  {tp:,}")

# Business metrics
print("\nBusiness Metrics:")
fraud_cost = 100  # Average cost of missing a fraud
false_alarm_cost = 5  # Cost of investigating false positive
fraud_losses = fn * fraud_cost
investigation_costs = fp * false_alarm_cost
total_cost = fraud_losses + investigation_costs
potential_losses = (tp + fn) * fraud_cost
savings = potential_losses - total_cost

print(f"Fraud Losses (Missed): ${fraud_losses:,.2f}")
print(f"Investigation Costs: ${investigation_costs:,.2f}")
print(f"Total Cost: ${total_cost:,.2f}")
print(f"Potential Losses: ${potential_losses:,.2f}")
print(f"Actual Savings: ${savings:,.2f}")
print(f"Savings Rate: {(savings/potential_losses)*100:.1f}%")

## 7. Visualizations

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# 1. Confusion Matrix Heatmap
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0],
            xticklabels=['Legitimate', 'Fraud'],
            yticklabels=['Legitimate', 'Fraud'])
axes[0,0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')
axes[0,0].set_ylabel('Actual')
axes[0,0].set_xlabel('Predicted')

# 2. ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
axes[0,1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.3f})')
axes[0,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')
axes[0,1].set_xlim([0.0, 1.0])
axes[0,1].set_ylim([0.0, 1.05])
axes[0,1].set_xlabel('False Positive Rate')
axes[0,1].set_ylabel('True Positive Rate')
axes[0,1].set_title('ROC Curve', fontsize=14, fontweight='bold')
axes[0,1].legend(loc='lower right')
axes[0,1].grid(alpha=0.3)

# 3. Precision-Recall Curve
precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
axes[1,0].plot(recall, precision, color='blue', lw=2, label=f'PR (AUC = {pr_auc:.3f})')
axes[1,0].axhline(y=y_test.mean(), color='red', linestyle='--', label='Baseline')
axes[1,0].set_xlim([0.0, 1.0])
axes[1,0].set_ylim([0.0, 1.05])
axes[1,0].set_xlabel('Recall')
axes[1,0].set_ylabel('Precision')
axes[1,0].set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')
axes[1,0].legend(loc='upper right')
axes[1,0].grid(alpha=0.3)

# 4. Prediction Distribution
axes[1,1].hist(y_pred_proba[y_test == 0], bins=50, alpha=0.7, 
              label='Legitimate', color='green', edgecolor='black')
axes[1,1].hist(y_pred_proba[y_test == 1], bins=50, alpha=0.7,
              label='Fraud', color='red', edgecolor='black')
axes[1,1].axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Threshold')
axes[1,1].set_xlabel('Predicted Fraud Probability')
axes[1,1].set_ylabel('Frequency')
axes[1,1].set_title('Prediction Distribution by Class', fontsize=14, fontweight='bold')
axes[1,1].legend()
axes[1,1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('../docs/baseline_performance.png', dpi=300, bbox_inches='tight')
plt.show()

## 8. Feature Importance Analysis

# Get feature coefficients
feature_importance = pd.DataFrame({
    'feature': X_train_baseline.columns,
    'coefficient': baseline_model.coef_[0]
})
feature_importance['abs_coefficient'] = feature_importance['coefficient'].abs()
feature_importance = feature_importance.sort_values('abs_coefficient', ascending=False)

print("\n" + "="*70)
print("TOP 15 MOST IMPORTANT FEATURES (by coefficient magnitude)")
print("="*70)
print(feature_importance.head(15).to_string(index=False))

# Plot top features
plt.figure(figsize=(10, 8))
top_features = feature_importance.head(15)
colors = ['red' if x < 0 else 'green' for x in top_features['coefficient']]
plt.barh(range(len(top_features)), top_features['coefficient'], color=colors, edgecolor='black')
plt.yticks(range(len(top_features)), top_features['feature'])
plt.xlabel('Coefficient Value')
plt.title('Top 15 Feature Importance (Logistic Regression Coefficients)', 
          fontsize=14, fontweight='bold')
plt.axvline(x=0, color='black', linestyle='-', linewidth=0.8)
plt.grid(axis='x', alpha=0.3)
plt.tight_layout()
plt.savefig('../docs/baseline_feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()

## 9. Cross-Validation

print("\n" + "="*70)
print("CROSS-VALIDATION ANALYSIS")
print("="*70)

# Perform 5-fold cross-validation
cv_scores_roc = cross_val_score(baseline_model, X_train_scaled, y_train, 
                                cv=5, scoring='roc_auc')

print(f"5-Fold Cross-Validation ROC-AUC Scores:")
for i, score in enumerate(cv_scores_roc, 1):
    print(f"  Fold {i}: {score:.4f}")
print(f"\nMean CV Score: {cv_scores_roc.mean():.4f} (+/- {cv_scores_roc.std() * 2:.4f})")

## 10. Baseline Summary & Next Steps

summary = f"""
{'='*70}
BASELINE MODEL SUMMARY
{'='*70}

### Model Configuration
- Algorithm: Logistic Regression with balanced class weights
- Features: 29 (V1-V28 + Amount, Time excluded)
- Preprocessing: StandardScaler
- Train Size: {len(X_train):,} samples
- Test Size: {len(X_test):,} samples

### Performance Metrics
- **ROC-AUC**: {roc_auc:.4f}
- **PR-AUC**: {pr_auc:.4f}
- **Precision**: {cm[1,1]/(cm[1,1]+cm[0,1]):.4f}
- **Recall**: {cm[1,1]/(cm[1,1]+cm[1,0]):.4f}
- **F1-Score**: {2*(cm[1,1]/(cm[1,1]+cm[0,1]))*(cm[1,1]/(cm[1,1]+cm[1,0]))/((cm[1,1]/(cm[1,1]+cm[0,1]))+(cm[1,1]/(cm[1,1]+cm[1,0]))):.4f}

### Business Impact
- True Positives: {tp} frauds caught
- False Negatives: {fn} frauds missed
- False Positives: {fp} false alarms
- Cost Savings: ${savings:,.2f}

### Key Observations
1. Model shows good discrimination ability (ROC-AUC > 0.90)
2. PR-AUC indicates room for improvement on precision-recall trade-off
3. V14, V17, and V12 are most important features
4. Class imbalance is partially handled by balanced weights

### Areas for Improvement
1. **Feature Engineering**: Add time-based and amount-based features
2. **Advanced Models**: Try XGBoost, LightGBM with better imbalance handling
3. **Threshold Tuning**: Optimize decision threshold for business objectives
4. **Sampling**: Apply SMOTE or other resampling techniques
5. **Hyperparameter Tuning**: Optimize model parameters

### Next Steps
→ Notebook 03: Feature Engineering to improve model inputs
→ Notebook 04: Model Optimization with advanced algorithms
→ Notebook 05: Evaluation and threshold selection

{'='*70}
"""

print(summary)

# Save summary to file
with open('../docs/baseline_results.md', 'w') as f:
    f.write("# Baseline Model Results\n\n")
    f.write(f"**Date**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
    f.write(summary)

print("\n✓ Baseline model completed successfully!")
print("✓ Results saved to ../docs/baseline_results.md")
print("✓ Visualizations saved to ../docs/")
