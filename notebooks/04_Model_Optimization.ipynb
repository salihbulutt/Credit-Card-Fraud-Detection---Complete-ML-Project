# Credit Card Fraud Detection - Model Optimization

"""
## Model Optimization & Hyperparameter Tuning

This notebook focuses on optimizing model hyperparameters to achieve
the best possible performance on the fraud detection task.

### Objectives:
1. Define hyperparameter search space
2. Use Optuna for Bayesian optimization
3. Compare different algorithms
4. Select best model configuration
5. Analyze parameter importance
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import optuna
from optuna.visualization import plot_optimization_history, plot_param_importances
import xgboost as xgb
import lightgbm as lgb
from sklearn.metrics import average_precision_score, roc_auc_score
import sys
sys.path.append('..')

from src.data_preprocessing import load_processed_data
from src.feature_engineering import engineer_features_pipeline
from src.model_trainer import ModelTrainer
from src.utils import calculate_metrics, print_metrics
import warnings
warnings.filterwarnings('ignore')

## 1. Load and Prepare Data

print("="*70)
print("MODEL OPTIMIZATION - HYPERPARAMETER TUNING")
print("="*70)

# Load data
X_train, X_val, X_test, y_train, y_val, y_test = load_processed_data()

# Engineer features
print("\nEngineering features...")
X_train_eng, X_val_eng, X_test_eng = engineer_features_pipeline(
    X_train, X_val, X_test
)

print(f"\nData shape: {X_train_eng.shape}")
print(f"Features: {X_train_eng.shape[1]}")

## 2. Baseline Model (from previous notebook)

print("\n" + "="*70)
print("BASELINE MODEL PERFORMANCE")
print("="*70)

# Train baseline XGBoost
baseline_params = {
    'n_estimators': 100,
    'max_depth': 4,
    'learning_rate': 0.1,
    'scale_pos_weight': 20,
    'random_state': 42
}

baseline_model = xgb.XGBClassifier(**baseline_params)
baseline_model.fit(X_train_eng, y_train)

y_val_pred_proba = baseline_model.predict_proba(X_val_eng)[:, 1]
y_val_pred = (y_val_pred_proba >= 0.5).astype(int)

baseline_metrics = calculate_metrics(y_val, y_val_pred, y_val_pred_proba)
print_metrics(baseline_metrics, "Baseline XGBoost")

baseline_pr_auc = baseline_metrics['pr_auc']

## 3. Hyperparameter Optimization with Optuna

print("\n" + "="*70)
print("HYPERPARAMETER OPTIMIZATION (OPTUNA)")
print("="*70)

def objective(trial):
    """Optuna objective function"""
    params = {
        'n_estimators': trial.suggest_categorical('n_estimators', [100, 200, 300, 500]),
        'max_depth': trial.suggest_int('max_depth', 3, 7),
        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'gamma': trial.suggest_float('gamma', 0, 2),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),
        'scale_pos_weight': trial.suggest_int('scale_pos_weight', 10, 30),
        'reg_alpha': trial.suggest_float('reg_alpha', 0, 0.5),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.5, 5.0),
        'random_state': 42,
        'eval_metric': 'aucpr'
    }
    
    model = xgb.XGBClassifier(**params)
    model.fit(
        X_train_eng, y_train,
        eval_set=[(X_val_eng, y_val)],
        early_stopping_rounds=20,
        verbose=False
    )
    
    y_pred_proba = model.predict_proba(X_val_eng)[:, 1]
    pr_auc = average_precision_score(y_val, y_pred_proba)
    
    return pr_auc

# Create study
study = optuna.create_study(
    direction='maximize',
    sampler=optuna.samplers.TPESampler(seed=42)
)

# Run optimization
print("\nRunning Optuna optimization (50 trials)...")
print("This may take 20-30 minutes...\n")
study.optimize(objective, n_trials=50, show_progress_bar=True)

# Best results
print("\n" + "="*70)
print("OPTIMIZATION RESULTS")
print("="*70)
print(f"Best PR-AUC: {study.best_value:.4f}")
print(f"Improvement over baseline: {((study.best_value - baseline_pr_auc) / baseline_pr_auc * 100):.2f}%")
print(f"\nBest parameters:")
for key, value in study.best_params.items():
    print(f"  {key:20s}: {value}")

## 4. Visualize Optimization Process

# Optimization history
fig = plot_optimization_history(study)
fig.update_layout(
    title="Hyperparameter Optimization History",
    xaxis_title="Trial Number",
    yaxis_title="PR-AUC Score"
)
fig.show()

# Parameter importances
fig = plot_param_importances(study)
fig.update_layout(title="Hyperparameter Importance")
fig.show()

# Parameter relationships
fig, axes = plt.subplots(2, 3, figsize=(18, 10))

important_params = ['learning_rate', 'max_depth', 'n_estimators', 
                   'scale_pos_weight', 'subsample', 'colsample_bytree']

for idx, param in enumerate(important_params):
    ax = axes[idx // 3, idx % 3]
    
    values = [trial.params[param] for trial in study.trials]
    scores = [trial.value for trial in study.trials]
    
    ax.scatter(values, scores, alpha=0.6)
    ax.set_xlabel(param)
    ax.set_ylabel('PR-AUC')
    ax.set_title(f'{param} vs PR-AUC')
    ax.grid(alpha=0.3)

plt.tight_layout()
plt.savefig('../docs/hyperparameter_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

## 5. Train Optimized Model

print("\n" + "="*70)
print("TRAINING OPTIMIZED MODEL")
print("="*70)

optimized_model = xgb.XGBClassifier(**study.best_params)
optimized_model.fit(
    X_train_eng, y_train,
    eval_set=[(X_val_eng, y_val)],
    early_stopping_rounds=20,
    verbose=True
)

# Evaluate
y_val_pred_proba_opt = optimized_model.predict_proba(X_val_eng)[:, 1]
y_val_pred_opt = (y_val_pred_proba_opt >= 0.5).astype(int)

optimized_metrics = calculate_metrics(y_val, y_val_pred_opt, y_val_pred_proba_opt)
print_metrics(optimized_metrics, "Optimized XGBoost")

## 6. Compare with LightGBM

print("\n" + "="*70)
print("COMPARING WITH LIGHTGBM")
print("="*70)

lgb_params = {
    'n_estimators': 300,
    'max_depth': 5,
    'learning_rate': 0.01,
    'num_leaves': 31,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'scale_pos_weight': 20,
    'random_state': 42,
    'verbose': -1
}

lgb_model = lgb.LGBMClassifier(**lgb_params)
lgb_model.fit(
    X_train_eng, y_train,
    eval_set=[(X_val_eng, y_val)],
    eval_metric='average_precision',
    callbacks=[lgb.early_stopping(20, verbose=False)]
)

y_val_pred_proba_lgb = lgb_model.predict_proba(X_val_eng)[:, 1]
y_val_pred_lgb = (y_val_pred_proba_lgb >= 0.5).astype(int)

lgb_metrics = calculate_metrics(y_val, y_val_pred_lgb, y_val_pred_proba_lgb)
print_metrics(lgb_metrics, "LightGBM")

## 7. Model Comparison

comparison_df = pd.DataFrame({
    'Baseline XGBoost': baseline_metrics,
    'Optimized XGBoost': optimized_metrics,
    'LightGBM': lgb_metrics
}).T

print("\n" + "="*70)
print("MODEL COMPARISON")
print("="*70)
print(comparison_df[['pr_auc', 'roc_auc', 'f1_score', 'precision', 'recall']])

# Visualize comparison
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Bar chart
comparison_df[['pr_auc', 'roc_auc', 'f1_score', 'precision', 'recall']].plot(
    kind='bar',
    ax=axes[0],
    width=0.8
)
axes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')
axes[0].set_ylabel('Score')
axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')
axes[0].legend(loc='lower right')
axes[0].grid(axis='y', alpha=0.3)

# Improvement percentages
improvements = ((comparison_df - comparison_df.loc['Baseline XGBoost']) / 
                comparison_df.loc['Baseline XGBoost'] * 100)
improvements = improvements.drop('Baseline XGBoost')

improvements[['pr_auc', 'roc_auc', 'f1_score', 'precision', 'recall']].plot(
    kind='bar',
    ax=axes[1],
    width=0.8
)
axes[1].set_title('% Improvement over Baseline', fontsize=14, fontweight='bold')
axes[1].set_ylabel('Improvement (%)')
axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')
axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.8)
axes[1].grid(axis='y', alpha=0.3)

plt.tight_layout()
plt.savefig('../docs/model_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

## 8. Save Best Model Info

best_model_info = {
    'model_type': 'XGBoost',
    'parameters': study.best_params,
    'pr_auc': float(optimized_metrics['pr_auc']),
    'roc_auc': float(optimized_metrics['roc_auc']),
    'f1_score': float(optimized_metrics['f1_score']),
    'precision': float(optimized_metrics['precision']),
    'recall': float(optimized_metrics['recall']),
    'improvement_vs_baseline': f"{((optimized_metrics['pr_auc'] - baseline_pr_auc) / baseline_pr_auc * 100):.2f}%"
}

# Save to markdown
with open('../docs/model_optimization.md', 'w') as f:
    f.write("# Model Optimization Results\n\n")
    f.write(f"**Date**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
    f.write("## Optimization Summary\n\n")
    f.write(f"- **Best Model**: {best_model_info['model_type']}\n")
    f.write(f"- **Best PR-AUC**: {best_model_info['pr_auc']:.4f}\n")
    f.write(f"- **Improvement**: {best_model_info['improvement_vs_baseline']}\n\n")
    f.write("## Best Parameters\n\n")
    for key, value in study.best_params.items():
        f.write(f"- **{key}**: {value}\n")
    f.write("\n## Performance Metrics\n\n")
    f.write(comparison_df.to_markdown())

print("\n✓ Model optimization completed!")
print("✓ Results saved to ../docs/model_optimization.md")
