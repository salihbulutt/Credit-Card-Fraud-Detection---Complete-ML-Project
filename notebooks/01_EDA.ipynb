# Credit Card Fraud Detection - Exploratory Data Analysis (EDA)

## 1. Problem Definition

"""
### Business Problem
Credit card fraud is a critical issue in the banking industry, causing significant financial losses 
and damaging customer trust. The goal is to build a machine learning model that can:

1. **Identify fraudulent transactions** in real-time
2. **Minimize false positives** to avoid blocking legitimate transactions
3. **Maximize fraud detection rate** to protect customers and the bank

### Dataset Information
- **Source**: Kaggle - Credit Card Fraud Detection
- **Size**: 284,807 transactions
- **Time Period**: 2 days in September 2013
- **Features**: 30 (28 PCA-transformed + Time + Amount)
- **Target**: Class (0 = Legitimate, 1 = Fraud)

### Key Challenges
- **Extreme Imbalance**: Only 0.172% fraud rate
- **PCA Features**: V1-V28 are anonymized, limiting domain interpretation
- **Real-time Requirements**: Model must be fast enough for production use
"""

# Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Set style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

# Load Data
df = pd.read_csv('../data/raw/creditcard.csv')

## 2. Initial Data Exploration

# Display basic information
print("="*70)
print("DATASET OVERVIEW")
print("="*70)
print(f"Shape: {df.shape}")
print(f"Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
print("\nColumn Data Types:")
print(df.dtypes.value_counts())

df.head()

# Check for missing values
print("\n" + "="*70)
print("MISSING VALUES CHECK")
print("="*70)
missing_values = df.isnull().sum()
if missing_values.sum() == 0:
    print("✓ No missing values found!")
else:
    print(missing_values[missing_values > 0])

# Check for duplicates
print("\n" + "="*70)
print("DUPLICATE CHECK")
print("="*70)
duplicates = df.duplicated().sum()
print(f"Duplicate rows: {duplicates}")

## 3. Target Variable Analysis

print("\n" + "="*70)
print("TARGET VARIABLE DISTRIBUTION")
print("="*70)

# Class distribution
class_counts = df['Class'].value_counts()
class_pct = df['Class'].value_counts(normalize=True) * 100

print(f"Legitimate Transactions (Class 0): {class_counts[0]:,} ({class_pct[0]:.3f}%)")
print(f"Fraudulent Transactions (Class 1): {class_counts[1]:,} ({class_pct[1]:.3f}%)")
print(f"\nImbalance Ratio: 1:{class_counts[0]//class_counts[1]}")

# Visualization
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Count plot
sns.countplot(data=df, x='Class', ax=axes[0])
axes[0].set_title('Transaction Class Distribution', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Class (0=Legit, 1=Fraud)')
axes[0].set_ylabel('Count')
for i, v in enumerate(class_counts):
    axes[0].text(i, v + 5000, f'{v:,}\n({class_pct[i]:.3f}%)', ha='center')

# Pie chart
colors = ['#4CAF50', '#F44336']
axes[1].pie(class_counts, labels=['Legitimate', 'Fraud'], autopct='%1.3f%%', 
            colors=colors, startangle=90)
axes[1].set_title('Percentage Distribution', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

## 4. Time Analysis

print("\n" + "="*70)
print("TIME ANALYSIS")
print("="*70)

# Time statistics
print(f"Time Range: {df['Time'].min():.0f} to {df['Time'].max():.0f} seconds")
print(f"Duration: {df['Time'].max() / 3600:.1f} hours ({df['Time'].max() / 86400:.1f} days)")

# Create time-based features for analysis
df['Hour'] = (df['Time'] / 3600) % 24
df['Day'] = df['Time'] // 86400

# Time distribution by class
fig, axes = plt.subplots(2, 2, figsize=(16, 10))

# Overall time distribution
axes[0, 0].hist(df['Time'] / 3600, bins=48, edgecolor='black', alpha=0.7)
axes[0, 0].set_title('Overall Transaction Distribution Over Time', fontsize=12, fontweight='bold')
axes[0, 0].set_xlabel('Time (hours)')
axes[0, 0].set_ylabel('Number of Transactions')

# Fraud vs Legitimate over time
for class_val, label, color in [(0, 'Legitimate', 'green'), (1, 'Fraud', 'red')]:
    class_data = df[df['Class'] == class_val]
    axes[0, 1].hist(class_data['Time'] / 3600, bins=48, alpha=0.6, 
                    label=label, color=color, edgecolor='black')
axes[0, 1].set_title('Transaction Distribution by Class', fontsize=12, fontweight='bold')
axes[0, 1].set_xlabel('Time (hours)')
axes[0, 1].set_ylabel('Number of Transactions')
axes[0, 1].legend()

# Hourly fraud rate
hourly_fraud = df.groupby('Hour')['Class'].agg(['sum', 'count'])
hourly_fraud['fraud_rate'] = (hourly_fraud['sum'] / hourly_fraud['count']) * 100

axes[1, 0].bar(hourly_fraud.index, hourly_fraud['fraud_rate'], color='coral', edgecolor='black')
axes[1, 0].set_title('Fraud Rate by Hour of Day', fontsize=12, fontweight='bold')
axes[1, 0].set_xlabel('Hour of Day')
axes[1, 0].set_ylabel('Fraud Rate (%)')
axes[1, 0].axhline(y=df['Class'].mean() * 100, color='red', linestyle='--', label='Average')
axes[1, 0].legend()

# Transaction count by hour and class
hour_class = df.groupby(['Hour', 'Class']).size().unstack(fill_value=0)
hour_class.plot(kind='bar', stacked=True, ax=axes[1, 1], color=['green', 'red'], alpha=0.7)
axes[1, 1].set_title('Transaction Count by Hour and Class', fontsize=12, fontweight='bold')
axes[1, 1].set_xlabel('Hour of Day')
axes[1, 1].set_ylabel('Number of Transactions')
axes[1, 1].legend(['Legitimate', 'Fraud'])
axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=45)

plt.tight_layout()
plt.show()

print("\nKey Time Insights:")
print(f"- Peak fraud hour: {hourly_fraud['fraud_rate'].idxmax():.0f}:00")
print(f"- Highest fraud rate: {hourly_fraud['fraud_rate'].max():.3f}%")
print(f"- Lowest fraud rate: {hourly_fraud['fraud_rate'].min():.3f}%")

## 5. Amount Analysis

print("\n" + "="*70)
print("TRANSACTION AMOUNT ANALYSIS")
print("="*70)

# Amount statistics by class
print("Legitimate Transactions:")
print(df[df['Class'] == 0]['Amount'].describe())
print("\nFraudulent Transactions:")
print(df[df['Class'] == 1]['Amount'].describe())

# Visualizations
fig, axes = plt.subplots(2, 3, figsize=(18, 10))

# Distribution plots
axes[0, 0].hist(df[df['Class'] == 0]['Amount'], bins=50, edgecolor='black', alpha=0.7, color='green')
axes[0, 0].set_title('Legitimate Transactions - Amount Distribution', fontsize=12, fontweight='bold')
axes[0, 0].set_xlabel('Amount ($)')
axes[0, 0].set_ylabel('Frequency')
axes[0, 0].set_xlim(0, 1000)

axes[0, 1].hist(df[df['Class'] == 1]['Amount'], bins=50, edgecolor='black', alpha=0.7, color='red')
axes[0, 1].set_title('Fraudulent Transactions - Amount Distribution', fontsize=12, fontweight='bold')
axes[0, 1].set_xlabel('Amount ($)')
axes[0, 1].set_ylabel('Frequency')

# Box plots
df.boxplot(column='Amount', by='Class', ax=axes[0, 2])
axes[0, 2].set_title('Amount Distribution by Class', fontsize=12, fontweight='bold')
axes[0, 2].set_xlabel('Class (0=Legit, 1=Fraud)')
axes[0, 2].set_ylabel('Amount ($)')
plt.sca(axes[0, 2])
plt.xticks([1, 2], ['Legitimate', 'Fraud'])

# Log-transformed distributions
axes[1, 0].hist(np.log1p(df[df['Class'] == 0]['Amount']), bins=50, alpha=0.7, color='green', edgecolor='black')
axes[1, 0].set_title('Legitimate - Log(Amount) Distribution', fontsize=12, fontweight='bold')
axes[1, 0].set_xlabel('Log(Amount + 1)')
axes[1, 0].set_ylabel('Frequency')

axes[1, 1].hist(np.log1p(df[df['Class'] == 1]['Amount']), bins=50, alpha=0.7, color='red', edgecolor='black')
axes[1, 1].set_title('Fraudulent - Log(Amount) Distribution', fontsize=12, fontweight='bold')
axes[1, 1].set_xlabel('Log(Amount + 1)')
axes[1, 1].set_ylabel('Frequency')

# Amount bins analysis
amount_bins = [0, 10, 50, 100, 500, 1000, 5000, df['Amount'].max()]
df['Amount_Bin'] = pd.cut(df['Amount'], bins=amount_bins)
bin_analysis = df.groupby(['Amount_Bin', 'Class']).size().unstack(fill_value=0)
bin_analysis['fraud_rate'] = (bin_analysis[1] / (bin_analysis[0] + bin_analysis[1])) * 100

bin_analysis['fraud_rate'].plot(kind='bar', ax=axes[1, 2], color='coral', edgecolor='black')
axes[1, 2].set_title('Fraud Rate by Amount Range', fontsize=12, fontweight='bold')
axes[1, 2].set_xlabel('Amount Range ($)')
axes[1, 2].set_ylabel('Fraud Rate (%)')
axes[1, 2].axhline(y=df['Class'].mean() * 100, color='red', linestyle='--', label='Overall Avg')
axes[1, 2].legend()
axes[1, 2].set_xticklabels(axes[1, 2].get_xticklabels(), rotation=45, ha='right')

plt.tight_layout()
plt.show()

print("\nKey Amount Insights:")
print(f"- Legitimate transactions: Mean=${df[df['Class']==0]['Amount'].mean():.2f}, Median=${df[df['Class']==0]['Amount'].median():.2f}")
print(f"- Fraudulent transactions: Mean=${df[df['Class']==1]['Amount'].mean():.2f}, Median=${df[df['Class']==1]['Amount'].median():.2f}")
print(f"- Small transactions (<$10) fraud rate: {(df[df['Amount'] < 10]['Class'].mean() * 100):.3f}%")
print(f"- Large transactions (>$1000) fraud rate: {(df[df['Amount'] > 1000]['Class'].mean() * 100):.3f}%")

## 6. PCA Features Analysis

print("\n" + "="*70)
print("PCA FEATURES ANALYSIS")
print("="*70)

# Select PCA features
pca_features = [f'V{i}' for i in range(1, 29)]

# Statistical summary for fraud vs legitimate
print("Feature means comparison (Fraud vs Legitimate):")
fraud_means = df[df['Class'] == 1][pca_features].mean()
legit_means = df[df['Class'] == 0][pca_features].mean()
mean_diff = (fraud_means - legit_means).abs().sort_values(ascending=False)

print("\nTop 10 features with largest mean difference:")
for feat in mean_diff.head(10).index:
    print(f"{feat}: Fraud={fraud_means[feat]:.3f}, Legit={legit_means[feat]:.3f}, Diff={mean_diff[feat]:.3f}")

# Visualize top discriminative features
top_features = mean_diff.head(6).index.tolist()

fig, axes = plt.subplots(2, 3, figsize=(18, 10))
axes = axes.flatten()

for idx, feature in enumerate(top_features):
    for class_val, label, color in [(0, 'Legitimate', 'green'), (1, 'Fraud', 'red')]:
        axes[idx].hist(df[df['Class'] == class_val][feature], bins=50, alpha=0.6,
                      label=label, color=color, edgecolor='black', density=True)
    axes[idx].set_title(f'{feature} Distribution', fontsize=12, fontweight='bold')
    axes[idx].set_xlabel(feature)
    axes[idx].set_ylabel('Density')
    axes[idx].legend()

plt.tight_layout()
plt.show()

## 7. Correlation Analysis

print("\n" + "="*70)
print("CORRELATION ANALYSIS")
print("="*70)

# Calculate correlations with target
correlations = df[pca_features + ['Amount']].corrwith(df['Class']).abs().sort_values(ascending=False)

print("Top 15 features correlated with fraud:")
print(correlations.head(15))

# Visualize correlations
fig, axes = plt.subplots(1, 2, figsize=(16, 6))

# Bar plot of correlations
correlations.head(15).plot(kind='barh', ax=axes[0], color='steelblue', edgecolor='black')
axes[0].set_title('Top 15 Features by Correlation with Fraud', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Absolute Correlation')
axes[0].invert_yaxis()

# Correlation heatmap for top features
top_corr_features = correlations.head(10).index.tolist() + ['Class']
corr_matrix = df[top_corr_features].corr()
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,
           ax=axes[1], cbar_kws={'label': 'Correlation'})
axes[1].set_title('Correlation Matrix - Top 10 Features', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.show()

## 8. Key EDA Findings

print("\n" + "="*70)
print("KEY EDA FINDINGS & INSIGHTS")
print("="*70)

findings = """
### 1. CLASS IMBALANCE
- Fraud rate: 0.172% (492 frauds out of 284,807 transactions)
- Imbalance ratio: 1:578 (extreme imbalance)
- **Action**: Must use techniques like SMOTE, class weights, or specialized metrics (PR-AUC)

### 2. TIME PATTERNS
- Transactions occur over 48 hours (2 days)
- Fraud rate varies by hour: Peak during night hours (0-6 AM)
- Time can be a useful feature for fraud detection
- **Action**: Create hour-of-day and time-period features

### 3. AMOUNT PATTERNS
- Fraudulent transactions: Lower average amount ($122) vs Legitimate ($88)
- Fraud distribution is more right-skewed
- Small transactions (<$10) have slightly higher fraud rate
- **Action**: Apply log transformation and create amount-based features

### 4. PCA FEATURES
- V14, V17, V12, V10, V11 show highest correlation with fraud
- V14 has the strongest discriminative power
- Most features have different distributions for fraud vs legitimate
- **Action**: All PCA features should be kept; they carry important information

### 5. MODELING RECOMMENDATIONS
- Primary metric: PR-AUC (Precision-Recall AUC) due to extreme imbalance
- Secondary metrics: ROC-AUC, F1-Score, Recall at high precision
- Validation strategy: Stratified K-Fold to maintain fraud rate
- Consider cost-sensitive learning (fraud misclassification is costly)

### 6. FEATURE ENGINEERING OPPORTUNITIES
- Time-based: hour_of_day, is_night, is_weekend, time_since_last_transaction
- Amount-based: log(amount), amount_z_score, amount_bins
- Interaction: V14 * V17, V12 * V14 (highly correlated features)
- Statistical: rolling means, standard deviations
"""

print(findings)

# Save findings to markdown file
with open('../docs/EDA_findings.md', 'w') as f:
    f.write("# Exploratory Data Analysis - Key Findings\n\n")
    f.write(f"**Analysis Date**: {pd.Timestamp.now().strftime('%Y-%m-%d')}\n\n")
    f.write(findings)

print("\n✓ EDA completed successfully!")
print("✓ Findings saved to ../docs/EDA_findings.md")
